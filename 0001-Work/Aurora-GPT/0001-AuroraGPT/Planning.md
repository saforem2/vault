# Planning

> [!tldr]+ Rick's Email
> Hi (and welcome to 2024!):  
>   
> Thanks to everyone that was able to participate in the face to face meeting Monday Dec 18th.  To keep the process  
> moving forward to building out the teams to help create AuroraGPT we wanted to collect a bit more information from  
> those that are interested in participating in the project.  
>   
> To recap, AuroraGPT is the Argonne led effort to build a series of LLMs aimed at scientific use cases.  The goal  
> is to explore the space of models that have more scientific capabilities than general purpose models being  
> developed by industry and if successful to create, evaluate and deploy such models (and their derivatives) for use by  
> ourselves, our partners and colleagues.   
> 
>   
> 
> AuroraGPT is an international effort, with a handful of participating partners.  Including  
> RIKEN in Japan and Barcelona Supercomputing Center in Spain.  
> 
>   
> 
> Others are also supporting the effort, such as Intel, HPE, Cerebras, etc..  
> 
>   
> 
> It is however a much smaller group than the “Trillion Parameter Consortium” which is a superset of academic and  
> commercial groups working towards very large AI models for science.   All the partners working on AuroraGPT  
> are part of TPC but many TPC members will not be directly involved in AuroraGPT.  
>   
> As we discussed in the meeting on the 18th, we are organizing the AuroraGPT effort around a set of working  
> groups (these are still being tweaked, but roughly):  
>   
> 1. Planning, Experiment Design, Neural Scaling, Resources and Repositories
> 2. Data Aggregation, Curation, and Data Prep Pipelines  
> 3. Models, Architecture, Performance Scaling, Inference Runtime, and Training Protocols
> 4. Model Evaluation and AI-Safety
> 5. Model Post-PreTraining, Instruct and Chat tuning
> 6. Model Deployment and Serving, Quantization, Inference Performance, Inference Runtime, APIs  
> 7. Model Distribution and Support
> 8. Project Communications, Documentation and Collaborator Engagement
> 
>   
> 
> We are also discussing a management model for the effort that would pair up Argonne co-leads for each of the working groups, to provide
> 
> some opportunity for leadership development and provide some structure for our collaborators to plug in.
> 
>   
> 
> To help with leadership and working group assignments, we would like to collect a bit more information from each person interested in
> 
> helping.. If you are more likely to not be directly involved, no need to respond to this note, we will keep the TPC-Argonne folks up to date on things as they progress.
> 
>   
> 
> Please take some time and repond to the following questions. (Send your responses to Rinku, Rick, Ian, Valerie, Salman, Charlie, and Mike)
> 
>   
> 
> A. What is your primary interest in the AuroraGPT effort?
> 
>   
> 
> B. How many hours per week are you currently using LLMs such as GPT4, Claude, LLama2, etc.?
> 
>   
> 
> C. Of the 8 working group topics which is of the most interest to you and why?
> 
>   
> 
> D. Please rank the working group topics in your order of preference to work on?
> 
>   
> 
> E. Please list a few applications you would like to use AuroraGPT for in the future?
> 
>   
> 
> F. Please outline a few of your skills and interests that might be particularly useful for the AuroraGPT effort?
> 
>   
> 
> G. If asked are you willing to be a co-lead of a working group?
> 
>   
> 
> H. What is one skill that you think a science oriented LLM needs to have that GPT4 doesn’t have currently?
> 
>   
> 
> I. What fraction of your time would be available to work on AuroraGPT if funding was available?
> 
>   
> 
> J. What fraction of your time would you be able to work on AuroraGPT if no funding for your effort was available (i.e. using other funding)?
> 
>   
> 
> Thanks for taking the time to respond and help us figure out they way forward.
> 
>   
> 
> Cheers,
> 
>   
> 
> —rick
> 
>   
> 
> --   
> Tpc-argonne mailing list  
> [Tpc-argonne@lists.cels.anl.gov](mailto:Tpc-argonne@lists.cels.anl.gov)  
> [https://lists.cels.anl.gov/mailman/listinfo/tpc-argonne](https://lists.cels.anl.gov/mailman/listinfo/tpc-argonne)


1. What is your primary interest in the AuroraGPT effort?
    2. optimized distributed training, parallelism efforts

3. How many hours per week are you currently using LLMs such as GPT4, Claude, LLama2, etc.?
    4. < 5 hours / week

5. Of the 8 working group topics which is of the most interest to you and why?
    6. Models, Architecture, Performance Scaling, Inference Runtime, and Training Protocols because this is where I have the most experience and would be able to contribute most effectively

7. Please rank the working group topics in your order of preference to work on?

    > 1. Models, Architecture, Performance Scaling, Inference Runtime, and Training Protocols
    > 2. Model Post-PreTraining, Instruct and Chat tuning
    > 3. Model Deployment and Serving, Quantization, Inference Performance, Inference Runtime, APIs  
    > 4. Planning, Experiment Design, Neural Scaling, Resources and Repositories
    > 5. Data Aggregation, Curation, and Data Prep Pipelines  
    > 6. Model Distribution and Support
    > 7. Model Evaluation and AI-Safety
    > 8. Project Communications, Documentation and Collaborator Engagement
    
9. Please list a few applications you would like to use AuroraGPT for in the future?
    10. I'd be interested to see how a scientifically trained LLM would perform at hypothesis generation and experiment planning for novel ideas.

11. Please outline a few of your skills and interests that might be particularly useful for the AuroraGPT effort?
    12. I've worked closely with the DeepSpeed team on optimizing LLM training performance at scale and have worked on various parallelism strategies for training large models.

13. If asked are you willing to be a co-lead of a working group?
    14. Yes.

15. What is one skill that you think a science oriented LLM needs to have that GPT4 doesn’t have currently?
    16. Self-consistency and the ability to reflect on its reasoning.

17. What fraction of your time would be available to work on AuroraGPT if funding was available?
    18. 50%+ assuming overlap with Aurora efforts

19. What fraction of your time would you be able to work on AuroraGPT if no funding for your effort was available (i.e. using other funding)?
    20. 50%+ assuming overlap with Aurora efforts
