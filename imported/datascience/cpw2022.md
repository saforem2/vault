---
title: Scaling Deep Learning Applications
date created: Saturday, May 7th 2022, 4:07:46 pm
date modified: Thursday, October 27th 2022, 7:52:19 pm
---
# Scaling Deep Learning Applications

Computational Performance Workshop @ ALCF 2022

**Author:** Sam Foreman ([foremans@anl.gov](mailto:///foremans@anl.gov))

This section of the workshop will introduce you to some of the tools that we use to run distributed deep learning training at ALCF.

**Note:** This topic was also covered at the [ALCF: Simulation, Data, and Learning Workshop for AI](https://github.com/argonne-lcf/sdl_ai_workshop), which has additional resources (+examples) for those interested.

---
## Outline
1. Introduction to Data Parallelism vs Model Parallelism


---
# Distributed Training
## Data Parallelism
- All of the workers own a replica of the model
- The global batch of data is split into multiple minibatches and processed by different workers
- Each worker computes the corresponding loss and gradients with respect to the data it processes
- Before the updating of the parameters at each epoch, the loss and gradients are averaged across all workers through a collective operation
    - Relatively simple to implement, `MPI_Allreduce` is the only communication operation required
- Our recent presentation on data-parallel training is available on [youtube](https://youtu.be/930yrXjNkgM)

![data-parallel|330](../../../Excalidraw/CPW-data-parallel.svg)

## Model Parallelism
- In this scheme, disjoint subsets of a neural network are assigned to different devices.
    - All computation associated with the subsets are distributed
- Communication happens between devices whenever there is dataflow between two subsets
- Suitable when the model is too large to fit onto a single device (CPU / GPU) because of the memory capacity
    - Partitioning the model into different subsets is not an easy task
    - Might potentially introduce load imbalancing issues limiting the scaling efficiency
![[./assets/cpw2022 2022-10-27-195011.excalidraw.svg|330]]

## Comparison
We can get a better understanding of the differences by looking at how both the data and model weights are distributed across the different workers as shown below.

In the **data parallel** approach, each worker receives the **complete** model with all of its weights, but only receives a **subset** of the data.

In the **model parallel** approach, each worker receives the **complete** data, but only receives a **subset** of the model weights. This can be useful when a model is too large to fit onto a single device (CPU / GPU).

![[Excalidraw/CPW-ParallelismSplits.svg]]

---
# TensorFlow with Horovod

**Note:** We provide a complete example here

Below we describe each of the steps necessary to use Horovod for distributed data-parallel training using `tensorflow >= 2.x`

**Goal:**
1. Understand how Horovod works with TensorFlow
2. Be able to modify existing code to be compatible with Horovod

**Steps**:

1. **Initialize Horovod**
    After this initialization, the rank ID and the number of processes can be referred to as `hvd.rank()` and `hvd.size()`, whereas `hvd.local_rank()` refers to the local rank ID within a node.

    This is useful when we are trying to assign GPUs to each rank
  ```python
  import horovod as hvd
  hvd.init()
  ```

2. **Assign GPUs to each rank**
    In this case, we set one GPU per process ID `hvd.local_rank()`
  ```python
  gpus = tf.config.experimental.list_physical_devices('GPU')
  for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
      if gpus:
          local_rank = hvd.local_rank()
          tf.config.experimental.set_visible_devices(gpus[local_rank], 'GPU')
  ```

3. **Scale the learning rate by the number of workers**
  ```python
  # Horovod: adjust learning rate based on number of GPUs
  optimizer = tf.optimizers.Adam(lr_init * hvd.size())
  ```

4. **Decorate our `train_step` with the `@tf.function` decorator:**
  ```python
  @tf.function
  def train_step(data, model, loss_fn, optimizer, first_batch, compress=True):
      batch, target = data
      with tf.GradientTape() as tape:
          output = model(batch, training=True)
          loss = loss_fn(target, output)
      compression = (
          hvd.Compression.fp16 if compress
          else hvd.Compression.none
      )
      # Wrap `tf.GradientTape` with `hvd.DistributedGradientTape`
      tape = hvd.DistributedGradientTape(tape, compression=compression)
      grads = tape.gradient(loss, model.trainable_variables)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))

      # Horovod: Broadcast initial variable states from rank 0 to all other
      # processes. This is necessary to ensure consistent initialization
      # of all workers when training is started with random weights or
      # restored from a checkpoint
      #
      # Note: broadcast should be done after the first gradient step
      # to ensure consistent optimizer initialization
      if first_batch:
          hvd.broadcast_variables(model.variables, root_rank=0)
          hvd.broadcast_variables(optimizer.variables, root_rank=0)

      return loss, output
  ```

5. **Checkpointing _only_ on root rank**
    It is important to let _only_ one process deal with the checkpointing file I/O to prevent a race condition
    ```python
    if hvd.rank() == 0:
        checkpoint.save(checkpoint_dir)
    ```


---

# ZeRO Data Parallelism

Modified from: https://huggingface.co/docs/transformers/parallelism

Consider the simple example of 3 layers, each with 3 weights (`Layer A` has `a0`, `a1`, and `a2`, etc.) as shown below

![[Excalidraw/CPW-Layers.svg]]


For input $\mathbf{x} = (x_{1}, x_{2})^{T}$, the output from each layer is then

$$
\mathbf{y}^{A} = \sigma\left(a_{0} + a_{1} x_{1} + a_{2} x_{2}\right)
$$

$$
\mathbf{y}^{B} = \sigma\left(b_{0} + b_{1} y^{A}_{1} + b_{2} y^{A}_{2}\right)
$$

$$
\mathbf{y}^{C} = \sigma\left(c_{0} + c_{1} y^{B}_{1} + c_{2} y^{B}_{2}\right)
$$


If we have 3 GPUs, the Sharded DDP (Zero-DDP) splits the model across them as

![[Excalidraw/CPW-Zero-DP-Split.svg]]


![[Excalidraw/CPW-LayerA-Inputs.svg|500]]

Our goal is to have all three GPUs compute the **full forward pass** on **their** mini-batch, without having to store the entire model on each GPU.

`GPU:0` has `a0` but needs `a1` from `GPU:1` and `a2` from `GPU:2`, as shown below

$$
\mathbf{y}^{A} = \sigma\left(a_{0} + a_{1} x_{1} + a_{2} x_{2}\right)
$$


Each GPU receives as input a minibatch of data and attempts to compute a forward pass of the model. If we first consider `GPU:0`, in order to compute the output from `Layer A`, it needs:

- `a0` ✅ on `GPU:0`
- `a1` ❌ on `GPU:1`
- `a2` ❌ on `GPU:2`

To compute the output from `LayerA` in the forward pass, we can summarize what each GPU has ✅ vs what it needs ❌

| <mark class="red">Layer A</mark>   | `GPU:0` | `GPU:1` | `GPU:2` |
| ---------------------------------- | ------- | ------- | ------- |
| `a0`                               | ✅      | ❌      | ❌      |
| `a1`                               | ❌      | ✅      | ❌      |
| `a2`                               | ❌      | ❌      | ✅      |
| <mark class="green">Layer B</mark> |         |         |         |
| `b0`                               | ✅      | ❌      | ❌      |
| `b1`                               | ❌      | ✅      | ❌      |
| `b2`                               | ❌      | ❌      | ✅      |


![[Excalidraw/CPW-LayerA-Sync.svg]]


Similarly, `GPU:1` has `a1` but needs `a0` from `GPU:0` and `a2` from `GPU:2` and similarly for `GPU:2`.
$$
\mathbf{y}^{B} = \sigma\left(b_{0} + b_{1} y^{A}_{1} + b_{2} y^{A}_{2}\right)
$$

We can perform all three of these communications in parallel as shown below

![[Excalidraw/CPW-LayerA-Sync.svg|390]] ![[Excalidraw/CPW-LayerB-Sync.svg|400]] ![[Excalidraw/CPW-LayerC-Sync.svg|390]]


![[Excalidraw/CWP-ZeRO-DataParallelism.svg]]
rk

Once all 3 GPUs have reconstructed `[a0, a1, a2]`, they can compute the forward pass for Layer A:

$$
\mathbf{y}^{A} = \sigma\left(a_{0} + a_{1} x_{1} + a_{2} x_{2} \right)
$$
